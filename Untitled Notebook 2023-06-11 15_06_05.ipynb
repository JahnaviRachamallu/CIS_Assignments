{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[1]\") \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\ndata = [(\"James\",\"Smith\",\"USA\",\"CA\"),(\"Michael\",\"Rose\",\"USA\",\"NY\"), \\\n    (\"Robert\",\"Williams\",\"USA\",\"CA\"),(\"Maria\",\"Jones\",\"USA\",\"FL\") \\\n  ]\ncolumns=[\"firstname\",\"lastname\",\"country\",\"state\"]\ndf=spark.createDataFrame(data=data,schema=columns)\nprint(df.collect())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"76bd84f4-cabf-4a26-bf55-1bce581a049a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|    James|   Smith|    USA|   CA|\n|  Michael|    Rose|    USA|   NY|\n|   Robert|Williams|    USA|   CA|\n|    Maria|   Jones|    USA|   FL|\n+---------+--------+-------+-----+\n\n[Row(firstname='James', lastname='Smith', country='USA', state='CA'), Row(firstname='Michael', lastname='Rose', country='USA', state='NY'), Row(firstname='Robert', lastname='Williams', country='USA', state='CA'), Row(firstname='Maria', lastname='Jones', country='USA', state='FL')]\n"]}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd    \ndata = [['Scott', 50], ['Jeff', 45], ['Thomas', 54],['Ann',34]] \n  \n# Create the pandas DataFrame \npandasDF = pd.DataFrame(data, columns = ['Name', 'Age']) \n  \n# print dataframe. \nprint(pandasDF)\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local[1]\") \\\n    .appName(\"SparkByExamples.com\") \\\n    .getOrCreate()\n\nsparkDF=spark.createDataFrame(pandasDF) \nsparkDF.printSchema()\nsparkDF.show()\n\n#sparkDF=spark.createDataFrame(pandasDF.astype(str)) \nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\nmySchema = StructType([ StructField(\"First Name\", StringType(), True)\\\n                       ,StructField(\"Age\", IntegerType(), True)])\n\nsparkDF2 = spark.createDataFrame(pandasDF,schema=mySchema)\nsparkDF2.printSchema()\nsparkDF2.show()\n\n\nspark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")\n\npandasDF2=sparkDF2.select(\"*\").toPandas\nprint(pandasDF2)\n\n\ntest=spark.conf.get(\"spark.sql.execution.arrow.enabled\")\nprint(test)\n\ntest123=spark.conf.get(\"spark.sql.execution.arrow.pyspark.fallback.enabled\")\nprint(test123)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"217d0a48-dd0d-43f1-97a5-f346dcf0f5d6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/plain":[],"application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["     Name  Age\n0   Scott   50\n1    Jeff   45\n2  Thomas   54\n3     Ann   34\nroot\n |-- Name: string (nullable = true)\n |-- Age: long (nullable = true)\n\n+------+---+\n|  Name|Age|\n+------+---+\n| Scott| 50|\n|  Jeff| 45|\n|Thomas| 54|\n|   Ann| 34|\n+------+---+\n\nroot\n |-- First Name: string (nullable = true)\n |-- Age: integer (nullable = true)\n\n+----------+---+\n|First Name|Age|\n+----------+---+\n|     Scott| 50|\n|      Jeff| 45|\n|    Thomas| 54|\n|       Ann| 34|\n+----------+---+\n\n<bound method PandasConversionMixin.toPandas of DataFrame[First Name: string, Age: int]>\ntrue\ntrue\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\ndata = [('James','Smith','M',3000),\n  ('Anna','Rose','F',4100),\n  ('Robert','Williams','M',6200), \n]\n\ncolumns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndf.show()\n\n\nif 'salary1' not in df.columns:\n    print(\"aa\")\n    \n# Add new constanct column\nfrom pyspark.sql.functions import lit\ndf.withColumn(\"bonus_percent\", lit(0.3)) \\\n  .show()\n  \n#Add column from existing column\ndf.withColumn(\"bonus_amount\", df.salary*0.3) \\\n  .show()\n\n#Add column by concatinating existing columns\nfrom pyspark.sql.functions import concat_ws\ndf.withColumn(\"name\", concat_ws(\",\",\"firstname\",'lastname')) \\\n  .show()\n\n#Add current date\nfrom pyspark.sql.functions import current_date\ndf.withColumn(\"current_date\", current_date()) \\\n  .show()\n\n\nfrom pyspark.sql.functions import when\ndf.withColumn(\"grade\", \\\n   when((df.salary < 4000), lit(\"A\")) \\\n     .when((df.salary >= 4000) & (df.salary <= 5000), lit(\"B\")) \\\n     .otherwise(lit(\"C\")) \\\n  ).show()\n    \n# Add column using select\ndf.select(\"firstname\",\"salary\", lit(0.3).alias(\"bonus\")).show()\ndf.select(\"firstname\",\"salary\", lit(df.salary * 0.3).alias(\"bonus_amount\")).show()\ndf.select(\"firstname\",\"salary\", current_date().alias(\"today_date\")).show()\n\n#Add columns using SQL\ndf.createOrReplaceTempView(\"PER\")\nspark.sql(\"select firstname,salary, '0.3' as bonus from PER\").show()\nspark.sql(\"select firstname,salary, salary * 0.3 as bonus_amount from PER\").show()\nspark.sql(\"select firstname,salary, current_date() as today_date from PER\").show()\nspark.sql(\"select firstname,salary, \" +\n          \"case salary when salary < 4000 then 'A' \"+\n          \"else 'B' END as grade from PER\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"51ef3a2b-3f0a-4c0e-a366-ee713dadfc5a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|  3000|\n|     Anna|    Rose|     F|  4100|\n|   Robert|Williams|     M|  6200|\n+---------+--------+------+------+\n\naa\n+---------+--------+------+------+-------------+\n|firstname|lastname|gender|salary|bonus_percent|\n+---------+--------+------+------+-------------+\n|    James|   Smith|     M|  3000|          0.3|\n|     Anna|    Rose|     F|  4100|          0.3|\n|   Robert|Williams|     M|  6200|          0.3|\n+---------+--------+------+------+-------------+\n\n+---------+--------+------+------+------------+\n|firstname|lastname|gender|salary|bonus_amount|\n+---------+--------+------+------+------------+\n|    James|   Smith|     M|  3000|       900.0|\n|     Anna|    Rose|     F|  4100|      1230.0|\n|   Robert|Williams|     M|  6200|      1860.0|\n+---------+--------+------+------+------------+\n\n+---------+--------+------+------+---------------+\n|firstname|lastname|gender|salary|           name|\n+---------+--------+------+------+---------------+\n|    James|   Smith|     M|  3000|    James,Smith|\n|     Anna|    Rose|     F|  4100|      Anna,Rose|\n|   Robert|Williams|     M|  6200|Robert,Williams|\n+---------+--------+------+------+---------------+\n\n+---------+--------+------+------+------------+\n|firstname|lastname|gender|salary|current_date|\n+---------+--------+------+------+------------+\n|    James|   Smith|     M|  3000|  2023-06-11|\n|     Anna|    Rose|     F|  4100|  2023-06-11|\n|   Robert|Williams|     M|  6200|  2023-06-11|\n+---------+--------+------+------+------------+\n\n+---------+--------+------+------+-----+\n|firstname|lastname|gender|salary|grade|\n+---------+--------+------+------+-----+\n|    James|   Smith|     M|  3000|    A|\n|     Anna|    Rose|     F|  4100|    B|\n|   Robert|Williams|     M|  6200|    C|\n+---------+--------+------+------+-----+\n\n+---------+------+-----+\n|firstname|salary|bonus|\n+---------+------+-----+\n|    James|  3000|  0.3|\n|     Anna|  4100|  0.3|\n|   Robert|  6200|  0.3|\n+---------+------+-----+\n\n+---------+------+------------+\n|firstname|salary|bonus_amount|\n+---------+------+------------+\n|    James|  3000|       900.0|\n|     Anna|  4100|      1230.0|\n|   Robert|  6200|      1860.0|\n+---------+------+------------+\n\n+---------+------+----------+\n|firstname|salary|today_date|\n+---------+------+----------+\n|    James|  3000|2023-06-11|\n|     Anna|  4100|2023-06-11|\n|   Robert|  6200|2023-06-11|\n+---------+------+----------+\n\n+---------+------+-----+\n|firstname|salary|bonus|\n+---------+------+-----+\n|    James|  3000|  0.3|\n|     Anna|  4100|  0.3|\n|   Robert|  6200|  0.3|\n+---------+------+-----+\n\n+---------+------+------------+\n|firstname|salary|bonus_amount|\n+---------+------+------------+\n|    James|  3000|       900.0|\n|     Anna|  4100|      1230.0|\n|   Robert|  6200|      1860.0|\n+---------+------+------------+\n\n+---------+------+----------+\n|firstname|salary|today_date|\n+---------+------+----------+\n|    James|  3000|2023-06-11|\n|     Anna|  4100|2023-06-11|\n|   Robert|  6200|2023-06-11|\n+---------+------+----------+\n\n+---------+------+-----+\n|firstname|salary|grade|\n+---------+------+-----+\n|    James|  3000|    B|\n|     Anna|  4100|    B|\n|   Robert|  6200|    B|\n+---------+------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a036550e-a0d5-45d4-9ae2-51c9ef9c20a2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n6\navg: 3400.0\n+------------------------------------------------------------+\n|collect_list(salary)                                        |\n+------------------------------------------------------------+\n|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n+------------------------------------------------------------+\n\n+------------------------------------+\n|collect_set(salary)                 |\n+------------------------------------+\n|[4600, 3000, 3900, 4100, 3300, 2000]|\n+------------------------------------+\n\n+----------------------------------+\n|count(DISTINCT department, salary)|\n+----------------------------------+\n|8                                 |\n+----------------------------------+\n\nDistinct Count of Department &amp; Salary: 8\ncount: Row(count(salary)=10)\n+-------------+\n|first(salary)|\n+-------------+\n|3000         |\n+-------------+\n\n+------------+\n|last(salary)|\n+------------+\n|4100        |\n+------------+\n\n+-------------------+\n|kurtosis(salary)   |\n+-------------------+\n|-0.6467803030303032|\n+-------------------+\n\n+-----------+\n|max(salary)|\n+-----------+\n|4600       |\n+-----------+\n\n+-----------+\n|min(salary)|\n+-----------+\n|2000       |\n+-----------+\n\n+-----------+\n|avg(salary)|\n+-----------+\n|3400.0     |\n+-----------+\n\n+--------------------+\n|skewness(salary)    |\n+--------------------+\n|-0.12041791181069571|\n+--------------------+\n\n+-------------------+-------------------+------------------+\n|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n+-------------------+-------------------+------------------+\n|765.9416862050705  |765.9416862050705  |726.636084983398  |\n+-------------------+-------------------+------------------+\n\n+-----------+\n|sum(salary)|\n+-----------+\n|34000      |\n+-----------+\n\n+--------------------+\n|sum(DISTINCT salary)|\n+--------------------+\n|20900               |\n+--------------------+\n\n+-----------------+-----------------+---------------+\n|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n+-----------------+-----------------+---------------+\n|586666.6666666666|586666.6666666666|528000.0       |\n+-----------------+-----------------+---------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import approx_count_distinct,collect_list\nfrom pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\nfrom pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \nfrom pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\nfrom pyspark.sql.functions import variance,var_samp,  var_pop\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nsimpleData = [(\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3000),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)\n  ]\nschema = [\"employee_name\", \"department\", \"salary\"]\n  \n  \ndf = spark.createDataFrame(data=simpleData, schema = schema)\ndf.printSchema()\ndf.show(truncate=False)\n\nprint(\"approx_count_distinct: \" + \\\n      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n\nprint(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n\ndf.select(collect_list(\"salary\")).show(truncate=False)\n\ndf.select(collect_set(\"salary\")).show(truncate=False)\n\ndf2 = df.select(countDistinct(\"department\", \"salary\"))\ndf2.show(truncate=False)\nprint(\"Distinct Count of Department &amp; Salary: \"+str(df2.collect()[0][0]))\n\nprint(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))\ndf.select(first(\"salary\")).show(truncate=False)\ndf.select(last(\"salary\")).show(truncate=False)\ndf.select(kurtosis(\"salary\")).show(truncate=False)\ndf.select(max(\"salary\")).show(truncate=False)\ndf.select(min(\"salary\")).show(truncate=False)\ndf.select(mean(\"salary\")).show(truncate=False)\ndf.select(skewness(\"salary\")).show(truncate=False)\ndf.select(stddev(\"salary\"), stddev_samp(\"salary\"), \\\n    stddev_pop(\"salary\")).show(truncate=False)\ndf.select(sum(\"salary\")).show(truncate=False)\ndf.select(sumDistinct(\"salary\")).show(truncate=False)\ndf.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")) \\\n  .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9728f3e5-5c9a-41a0-85a2-cfeb3023cffd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\napprox_count_distinct: 6\navg: 3400.0\n+------------------------------------------------------------+\n|collect_list(salary)                                        |\n+------------------------------------------------------------+\n|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n+------------------------------------------------------------+\n\n+------------------------------------+\n|collect_set(salary)                 |\n+------------------------------------+\n|[4600, 3000, 3900, 4100, 3300, 2000]|\n+------------------------------------+\n\n+----------------------------------+\n|count(DISTINCT department, salary)|\n+----------------------------------+\n|8                                 |\n+----------------------------------+\n\nDistinct Count of Department &amp; Salary: 8\ncount: Row(count(salary)=10)\n+-------------+\n|first(salary)|\n+-------------+\n|3000         |\n+-------------+\n\n+------------+\n|last(salary)|\n+------------+\n|4100        |\n+------------+\n\n+-------------------+\n|kurtosis(salary)   |\n+-------------------+\n|-0.6467803030303032|\n+-------------------+\n\n+-----------+\n|max(salary)|\n+-----------+\n|4600       |\n+-----------+\n\n+-----------+\n|min(salary)|\n+-----------+\n|2000       |\n+-----------+\n\n+-----------+\n|avg(salary)|\n+-----------+\n|3400.0     |\n+-----------+\n\n+--------------------+\n|skewness(salary)    |\n+--------------------+\n|-0.12041791181069571|\n+--------------------+\n\n+-------------------+-------------------+------------------+\n|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n+-------------------+-------------------+------------------+\n|765.9416862050705  |765.9416862050705  |726.636084983398  |\n+-------------------+-------------------+------------------+\n\n+-----------+\n|sum(salary)|\n+-----------+\n|34000      |\n+-----------+\n\n"]},{"output_type":"stream","output_type":"stream","name":"stderr","text":["/databricks/spark/python/pyspark/sql/functions.py:723: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"]},{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+\n|sum(DISTINCT salary)|\n+--------------------+\n|20900               |\n+--------------------+\n\n+-----------------+-----------------+---------------+\n|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n+-----------------+-----------------+---------------+\n|586666.6666666666|586666.6666666666|528000.0       |\n+-----------------+-----------------+---------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[1]\") \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\ncolumns = [\"name\",\"languagesAtSchool\",\"currentState\"]\ndata = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n\ndf = spark.createDataFrame(data=data,schema=columns)\ndf.printSchema()\ndf.show(truncate=False)\nfrom pyspark.sql.functions import col, concat_ws\ndf2 = df.withColumn(\"languagesAtSchool\",\n   concat_ws(\",\",col(\"languagesAtSchool\")))\ndf2.printSchema()\ndf2.show(truncate=False)\n\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ea90bf6b-89fc-430c-8568-735fb8d80390","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\n+----------------+------------------+------------+\n|name            |languagesAtSchool |currentState|\n+----------------+------------------+------------+\n|James,,Smith    |[Java, Scala, C++]|CA          |\n|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n|Robert,,Williams|[CSharp, VB]      |NV          |\n+----------------+------------------+------------+\n\nroot\n |-- name: string (nullable = true)\n |-- languagesAtSchool: string (nullable = false)\n |-- currentState: string (nullable = true)\n\n+----------------+-----------------+------------+\n|name            |languagesAtSchool|currentState|\n+----------------+-----------------+------------+\n|James,,Smith    |Java,Scala,C++   |CA          |\n|Michael,Rose,   |Spark,Java,C++   |NJ          |\n|Robert,,Williams|CSharp,VB        |NV          |\n+----------------+-----------------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["\n "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"52832d7a-3909-4243-a92e-ff6b13d5a4a9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|  3000|\n|     Anna|    Rose|     F|  4100|\n|   Robert|Williams|     M|  6200|\n+---------+--------+------+------+\n\naa\n+---------+--------+------+------+-------------+\n|firstname|lastname|gender|salary|bonus_percent|\n+---------+--------+------+------+-------------+\n|    James|   Smith|     M|  3000|          0.3|\n|     Anna|    Rose|     F|  4100|          0.3|\n|   Robert|Williams|     M|  6200|          0.3|\n+---------+--------+------+------+-------------+\n\n+---------+--------+------+------+------------+\n|firstname|lastname|gender|salary|bonus_amount|\n+---------+--------+------+------+------------+\n|    James|   Smith|     M|  3000|       900.0|\n|     Anna|    Rose|     F|  4100|      1230.0|\n|   Robert|Williams|     M|  6200|      1860.0|\n+---------+--------+------+------+------------+\n\n+---------+--------+------+------+---------------+\n|firstname|lastname|gender|salary|           name|\n+---------+--------+------+------+---------------+\n|    James|   Smith|     M|  3000|    James,Smith|\n|     Anna|    Rose|     F|  4100|      Anna,Rose|\n|   Robert|Williams|     M|  6200|Robert,Williams|\n+---------+--------+------+------+---------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StringType, ArrayType,StructType,StructField\nspark = SparkSession.builder \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\n\narrayCol = ArrayType(StringType(),False)\n\ndata = [\n (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n]\n\nschema = StructType([ \n    StructField(\"name\",StringType(),True), \n    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n    StructField(\"currentState\", StringType(), True), \n    StructField(\"previousState\", StringType(), True) \n  ])\n\ndf = spark.createDataFrame(data=data,schema=schema)\ndf.printSchema()\ndf.show()\n\nfrom pyspark.sql.functions import explode\ndf.select(df.name,explode(df.languagesAtSchool)).show()\n\nfrom pyspark.sql.functions import split\ndf.select(split(df.name,\",\").alias(\"nameAsArray\")).show()\n\nfrom pyspark.sql.functions import array\ndf.select(df.name,array(df.currentState,df.previousState).alias(\"States\")).show()\n\nfrom pyspark.sql.functions import array_contains\ndf.select(df.name,array_contains(df.languagesAtSchool,\"Java\")\n    .alias(\"array_contains\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2cb70be6-710c-4193-825e-4f36409a7404","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- languagesAtWork: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n |-- previousState: string (nullable = true)\n\n+----------------+------------------+---------------+------------+-------------+\n|            name| languagesAtSchool|languagesAtWork|currentState|previousState|\n+----------------+------------------+---------------+------------+-------------+\n|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|\n|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|\n|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|\n+----------------+------------------+---------------+------------+-------------+\n\n+----------------+------+\n|            name|   col|\n+----------------+------+\n|    James,,Smith|  Java|\n|    James,,Smith| Scala|\n|    James,,Smith|   C++|\n|   Michael,Rose,| Spark|\n|   Michael,Rose,|  Java|\n|   Michael,Rose,|   C++|\n|Robert,,Williams|CSharp|\n|Robert,,Williams|    VB|\n+----------------+------+\n\n+--------------------+\n|         nameAsArray|\n+--------------------+\n|    [James, , Smith]|\n|   [Michael, Rose, ]|\n|[Robert, , Williams]|\n+--------------------+\n\n+----------------+--------+\n|            name|  States|\n+----------------+--------+\n|    James,,Smith|[OH, CA]|\n|   Michael,Rose,|[NY, NJ]|\n|Robert,,Williams|[UT, NV]|\n+----------------+--------+\n\n+----------------+--------------+\n|            name|array_contains|\n+----------------+--------------+\n|    James,,Smith|          true|\n|   Michael,Rose,|          true|\n|Robert,,Williams|         false|\n+----------------+--------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nstates = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\nbroadcastStates = spark.sparkContext.broadcast(states)\n\ndata = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n  ]\n\ncolumns = [\"firstname\",\"lastname\",\"country\",\"state\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\ndef state_convert(code):\n    return broadcastStates.value[code]\n\nresult = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\nresult.show(truncate=False)\n\n# Broadcast variable on filter\nfilteDf= df.where((df['state'].isin(broadcastStates.value)))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e40affac-8656-4a70-9637-5c677d716be5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- country: string (nullable = true)\n |-- state: string (nullable = true)\n\n+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|James    |Smith   |USA    |CA   |\n|Michael  |Rose    |USA    |NY   |\n|Robert   |Williams|USA    |CA   |\n|Maria    |Jones   |USA    |FL   |\n+---------+--------+-------+-----+\n\n+---------+--------+-------+----------+\n|firstname|lastname|country|state     |\n+---------+--------+-------+----------+\n|James    |Smith   |USA    |California|\n|Michael  |Rose    |USA    |New York  |\n|Robert   |Williams|USA    |California|\n|Maria    |Jones   |USA    |Florida   |\n+---------+--------+-------+----------+\n\nUnexpected exception formatting exception. Falling back to standard exception\n"]},{"output_type":"stream","output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<command-3587618399680906>\", line 28, in <module>\n    filteDf= df.where((df['state'].isin(broadcastStates.value)))\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 48, in wrapper\n    res = func(*args, **kwargs)\n  File \"/databricks/spark/python/pyspark/sql/column.py\", line 904, in isin\n    [c._jc if isinstance(c, Column) else _create_column_from_literal(c) for c in cols],\n  File \"/databricks/spark/python/pyspark/sql/column.py\", line 904, in <listcomp>\n    [c._jc if isinstance(c, Column) else _create_column_from_literal(c) for c in cols],\n  File \"/databricks/spark/python/pyspark/sql/column.py\", line 51, in _create_column_from_literal\n    return sc._jvm.functions.lit(literal)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/errors/exceptions.py\", line 228, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.sql.functions.lit.\n: org.apache.spark.SparkRuntimeException: [UNSUPPORTED_FEATURE.LITERAL_TYPE] The feature is not supported: Literal for '{FL=Florida, NY=New York, CA=California}' of class java.util.HashMap.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.literalTypeUnsupportedError(QueryExecutionErrors.scala:375)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:103)\n\tat org.apache.spark.sql.functions$.lit(functions.scala:128)\n\tat org.apache.spark.sql.functions.lit(functions.scala)\n\tat sun.reflect.GeneratedMethodAccessor807.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n    return FormattedTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n    return VerboseTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n    frames.append(self.format_record(r))\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 730, in format_record\n    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n    pieces = self.included_pieces\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n    pos = scope_pieces.index(self.executing_piece)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n    return only(\n  File \"/databricks/python/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n    raise NotOneValueFound('Expected one value, found 0')\nexecuting.executing.NotOneValueFound: Expected one value, found 0\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.sql.functions.lit.\n: org.apache.spark.SparkRuntimeException: [UNSUPPORTED_FEATURE.LITERAL_TYPE] The feature is not supported: Literal for '{FL=Florida, NY=New York, CA=California}' of class java.util.HashMap.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.literalTypeUnsupportedError(QueryExecutionErrors.scala:375)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:103)\n\tat org.apache.spark.sql.functions$.lit(functions.scala:128)\n\tat org.apache.spark.sql.functions.lit(functions.scala)\n\tat sun.reflect.GeneratedMethodAccessor807.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b2c5513d-224a-497a-bd6e-ac653737a136","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Untitled Notebook 2023-06-11 15:06:05","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
