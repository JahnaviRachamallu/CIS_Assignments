{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession,Row\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata=[(\"James\",23),(\"Ann\",40)]\ndf=spark.createDataFrame(data).toDF(\"name.fname\",\"gender\")\ndf.printSchema()\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a5962967-0753-4276-85a8-665aef9ab57d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name.fname: string (nullable = true)\n |-- gender: long (nullable = true)\n\n+----------+------+\n|name.fname|gender|\n+----------+------+\n|     James|    23|\n|       Ann|    40|\n+----------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\ndf=spark.createDataFrame(data)\ndf.printSchema()\ndf.show(truncate=False)\n\ndf.select(df.prop.hair).show()\ndf.select(df[\"prop.hair\"]).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"33bda051-05b1-473f-bbba-903efbba47fe","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- prop: struct (nullable = true)\n |    |-- hair: string (nullable = true)\n |    |-- eye: string (nullable = true)\n\n+-----+-------------+\n|name |prop         |\n+-----+-------------+\n|James|{black, blue}|\n|Ann  |{grey, black}|\n+-----+-------------+\n\n+---------+\n|prop.hair|\n+---------+\n|    black|\n|     grey|\n+---------+\n\n+-----+\n| hair|\n+-----+\n|black|\n| grey|\n+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data=[(100,2,1),(200,3,4),(300,4,4)]\ndf=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\ndf.show(truncate=False)\ndf.select(df.col1 + df.col2).show()\ndf.select(df.col1 - df.col2).show() \ndf.select(df.col1 * df.col2).show()\ndf.select(df.col1 / df.col2).show()\ndf.select(df.col1 % df.col2).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f0f2602b-9f45-4a64-8fe3-bc21436c74b5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+----+----+\n|col1|col2|col3|\n+----+----+----+\n|100 |2   |1   |\n|200 |3   |4   |\n|300 |4   |4   |\n+----+----+----+\n\n+-------------+\n|(col1 + col2)|\n+-------------+\n|          102|\n|          203|\n|          304|\n+-------------+\n\n+-------------+\n|(col1 - col2)|\n+-------------+\n|           98|\n|          197|\n|          296|\n+-------------+\n\n+-------------+\n|(col1 * col2)|\n+-------------+\n|          200|\n|          600|\n|         1200|\n+-------------+\n\n+-----------------+\n|    (col1 / col2)|\n+-----------------+\n|             50.0|\n|66.66666666666667|\n|             75.0|\n+-----------------+\n\n+-------------+\n|(col1 % col2)|\n+-------------+\n|            0|\n|            2|\n|            0|\n+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndataDictionary = [\n        ('James',{'hair':'black','eye':'brown'}),\n        ('Michael',{'hair':'brown','eye':None}),\n        ('Robert',{'hair':'red','eye':'black'}),\n        ('Washington',{'hair':'grey','eye':'grey'}),\n        ('Jefferson',{'hair':'brown','eye':''})\n        ]\n\ndf = spark.createDataFrame(data=dataDictionary, schema = ['name','properties'])\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"809928e4-a92f-4899-b5f5-060542eabdc6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-----------------------------+\n|name      |properties                   |\n+----------+-----------------------------+\n|James     |{eye -> brown, hair -> black}|\n|Michael   |{eye -> null, hair -> brown} |\n|Robert    |{eye -> black, hair -> red}  |\n|Washington|{eye -> grey, hair -> grey}  |\n|Jefferson |{eye -> , hair -> brown}     |\n+----------+-----------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df3=df.rdd.map(lambda x: \\\n    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n    .toDF([\"name\",\"hair\",\"eye\"])\ndf3.printSchema()\ndf3.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5be33b49-c575-4875-862a-839663c9e17d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- hair: string (nullable = true)\n |-- eye: string (nullable = true)\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"hair\",df.properties[\"hair\"]) \\\n  .withColumn(\"eye\",df.properties[\"eye\"]) \\\n  .drop(\"properties\") \\\n  .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7c137185-2f51-49c4-b555-e1d450566e5b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"hair\",df.properties[\"hair\"]) \\\n  .withColumn(\"eye\",df.properties[\"eye\"]) \\\n  .drop(\"properties\") \\\n  .show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"141aeb3d-e8a8-4632-9c7d-74321b6739ff","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import explode,map_keys,col\nkeysDF = df.select(explode(map_keys(df.properties))).distinct()\nkeysList = keysDF.rdd.map(lambda x:x[0]).collect()\nkeyCols = list(map(lambda x: col(\"properties\").getItem(x).alias(str(x)), keysList))\ndf.select(df.name, *keyCols).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ed0e2c70-d566-40f7-b004-ed80a1d9e449","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+-----+-----+\n|      name|  eye| hair|\n+----------+-----+-----+\n|     James|brown|black|\n|   Michael| null|brown|\n|    Robert|black|  red|\n|Washington| grey| grey|\n| Jefferson|     |brown|\n+----------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import explode,map_keys,col\nkeysDF = df.select(explode(map_keys(df.properties))).distinct()\nkeysList = keysDF.rdd.map(lambda x:x[0]).collect()\nkeyCols = list(map(lambda x: col(\"properties\").getItem(x).alias(str(x)), keysList))\ndf.select(df.name, *keyCols).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"03dddf0b-1e10-41fb-a986-3ad4bb375f8a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+-----+-----+\n|      name|  eye| hair|\n+----------+-----+-----+\n|     James|brown|black|\n|   Michael| null|brown|\n|    Robert|black|  red|\n|Washington| grey| grey|\n| Jefferson|     |brown|\n+----------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [ (\"36636\",\"Finance\",3000,\"USA\"), \n    (\"40288\",\"Finance\",5000,\"IND\"), \n    (\"42114\",\"Sales\",3900,\"USA\"), \n    (\"39192\",\"Marketing\",2500,\"CAN\"), \n    (\"34534\",\"Sales\",6500,\"USA\") ]\nschema = StructType([\n     StructField('id', StringType(), True),\n     StructField('dept', StringType(), True),\n     StructField('salary', IntegerType(), True),\n     StructField('location', StringType(), True)\n     ])\n\ndf = spark.createDataFrame(data=data,schema=schema)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"cf560bb7-6917-4118-9236-444c1326eb5a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- location: string (nullable = true)\n\n+-----+---------+------+--------+\n|id   |dept     |salary|location|\n+-----+---------+------+--------+\n|36636|Finance  |3000  |USA     |\n|40288|Finance  |5000  |IND     |\n|42114|Sales    |3900  |USA     |\n|39192|Marketing|2500  |CAN     |\n|34534|Sales    |6500  |USA     |\n+-----+---------+------+--------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col,lit,create_map\ndf = df.withColumn(\"propertiesMap\",create_map(\n        lit(\"salary\"),col(\"salary\"),\n        lit(\"location\"),col(\"location\")\n        )).drop(\"salary\",\"location\")\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b029e173-5172-41a0-9357-094c145dec47","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- propertiesMap: map (nullable = false)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+-----+---------+---------------------------------+\n|id   |dept     |propertiesMap                    |\n+-----+---------+---------------------------------+\n|36636|Finance  |{salary -> 3000, location -> USA}|\n|40288|Finance  |{salary -> 5000, location -> IND}|\n|42114|Sales    |{salary -> 3900, location -> USA}|\n|39192|Marketing|{salary -> 2500, location -> CAN}|\n|34534|Sales    |{salary -> 6500, location -> USA}|\n+-----+---------+---------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n         .appName('SparkByExamples.com') \\\n         .getOrCreate()\n\ndata = [(\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3040),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)\n  ]\ncolumns = [\"Name\",\"Dept\",\"Salary\"]\ndf = spark.createDataFrame(data=data,schema=columns)\ndf.distinct().show()\nprint(df.distinct().count())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0a411403-535e-48ef-8246-d00340605520","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+---------+------+\n|   Name|     Dept|Salary|\n+-------+---------+------+\n|  James|    Sales|  3000|\n|Michael|    Sales|  4600|\n| Robert|    Sales|  4100|\n|  Maria|  Finance|  3000|\n|  James|    Sales|  3040|\n|  Scott|  Finance|  3300|\n|    Jen|  Finance|  3900|\n|   Jeff|Marketing|  3000|\n|  Kumar|Marketing|  2000|\n|   Saif|    Sales|  4100|\n+-------+---------+------+\n\n10\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct\ndf2=df.select(countDistinct(\"Dept\",\"Salary\"))\ndf2.show()\nprint(df2.collect()[0][0])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"728cf54a-dbb1-429c-8ee3-e5a037a78317","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------------------------+\n|count(DISTINCT Dept, Salary)|\n+----------------------------+\n|                           9|\n+----------------------------+\n\n9\n"]}],"execution_count":0},{"cell_type":"code","source":["df.createOrReplaceTempView(\"PERSON\")\nspark.sql(\"select distinct(count(*)) from PERSON\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9e3958b5-8136-4a2d-89ee-ea9aff13a766","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------+\n|count(1)|\n+--------+\n|      10|\n+--------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndataDictionary = [\n        ('James',{'hair':'black','eye':'brown'}),\n        ('Michael',{'hair':'brown','eye':None}),\n        ('Robert',{'hair':'red','eye':'black'}),\n        ('Washington',{'hair':'grey','eye':'grey'}),\n        ('Jefferson',{'hair':'brown','eye':''})\n        ]\n\ndf = spark.createDataFrame(data=dataDictionary, schema = ['name','properties'])\ndf.printSchema()\ndf.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"099c481a-ce75-4318-ac3d-cd68ef5ade87","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-----------------------------+\n|name      |properties                   |\n+----------+-----------------------------+\n|James     |{eye -> brown, hair -> black}|\n|Michael   |{eye -> null, hair -> brown} |\n|Robert    |{eye -> black, hair -> red}  |\n|Washington|{eye -> grey, hair -> grey}  |\n|Jefferson |{eye -> , hair -> brown}     |\n+----------+-----------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StructField, StructType, StringType, MapType,IntegerType\nschema = StructType([\n    StructField('name', StringType(), True),\n    StructField('properties', MapType(StringType(),StringType()),True)\n])\ndf2 = spark.createDataFrame(data=dataDictionary, schema = schema)\ndf2.printSchema()\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9f8406c4-ad2c-496c-9f5d-01aed2a584c6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-----------------------------+\n|name      |properties                   |\n+----------+-----------------------------+\n|James     |{eye -> brown, hair -> black}|\n|Michael   |{eye -> null, hair -> brown} |\n|Robert    |{eye -> black, hair -> red}  |\n|Washington|{eye -> grey, hair -> grey}  |\n|Jefferson |{eye -> , hair -> brown}     |\n+----------+-----------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df3=df.rdd.map(lambda x: \\\n    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n    .toDF([\"name\",\"hair\",\"eye\"])\ndf3.printSchema()\ndf3.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"dc38fbb4-65bb-4234-be41-f172174af9ac","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- hair: string (nullable = true)\n |-- eye: string (nullable = true)\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"439af373-d579-4819-8fbb-ec8d854cd280","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Untitled Notebook 2023-06-17 22:15:23","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
