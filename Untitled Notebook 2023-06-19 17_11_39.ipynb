{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import current_date, lit, when\n\n# Create a Spark session\nspark = SparkSession.builder.getOrCreate()\n\n# Sample data with customer information\ndata = [\n    (1, 'John Doe', '123 Main St', '555-1234', '2021-01-01'),\n    (2, 'Jane Smith', '456 Elm St', '555-5678', '2021-01-01'),\n    (3, 'Bob Johnson', '789 Oak St', '555-9090', '2021-01-01')\n]\n\n# Create an initial DataFrame\ndf = spark.createDataFrame(data, ['customer_id', 'name', 'address', 'phone', 'effective_date'])\n\n# New data to update the address for customer_id 2\nnew_data = [\n    (2, 'Jane Smith', '789 Pine St', '555-5678', '2023-06-19')\n]\nnew_df = spark.createDataFrame(new_data, ['customer_id', 'name', 'address', 'phone', 'effective_date'])\ndf = df.withColumn('current', lit(True))\ndf.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"acffd709-5b8e-4bb4-9676-698e53fb1e4c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1361810498471495>:23\u001B[0m\n\u001B[1;32m     21\u001B[0m new_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(new_data, [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcustomer_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124maddress\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mphone\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meffective_date\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     22\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcurrent\u001B[39m\u001B[38;5;124m'\u001B[39m, lit(\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[0;32m---> 23\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcurrent\u001B[39m\u001B[38;5;124m'\u001B[39m, when(df\u001B[38;5;241m.\u001B[39mcustomer_id\u001B[38;5;241m.\u001B[39misin(new_df\u001B[38;5;241m.\u001B[39mcustomer_id), \u001B[38;5;28;01mFalse\u001B[39;00m)\u001B[38;5;241m.\u001B[39motherwise(df\u001B[38;5;241m.\u001B[39mcurrent))\n\u001B[1;32m     24\u001B[0m df\u001B[38;5;241m.\u001B[39mcollect()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4758\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   4753\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n\u001B[1;32m   4754\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   4755\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   4756\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   4757\u001B[0m     )\n\u001B[0;32m-> 4758\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Resolved attribute(s) customer_id#121L missing from customer_id#111L,name#112,address#113,phone#114,effective_date#115,current#131 in operator !Project [customer_id#111L, name#112, address#113, phone#114, effective_date#115, CASE WHEN customer_id#111L IN (customer_id#121L) THEN false ELSE current#131 END AS current#138]. Attribute(s) with the same name appear in the operation: customer_id. Please check if the right attribute(s) are used.;\n!Project [customer_id#111L, name#112, address#113, phone#114, effective_date#115, CASE WHEN customer_id#111L IN (customer_id#121L) THEN false ELSE current#131 END AS current#138]\n+- Project [customer_id#111L, name#112, address#113, phone#114, effective_date#115, true AS current#131]\n   +- LogicalRDD [customer_id#111L, name#112, address#113, phone#114, effective_date#115], false\n","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Resolved attribute(s) customer_id#121L missing from customer_id#111L,name#112,address#113,phone#114,effective_date#115,current#131 in operator !Project [customer_id#111L, name#112, address#113, phone#114, effective_date#115, CASE WHEN customer_id#111L IN (customer_id#121L) THEN false ELSE current#131 END AS current#138]. Attribute(s) with the same name appear in the operation: customer_id. Please check if the right attribute(s) are used.;\n!Project [customer_id#111L, name#112, address#113, phone#114, effective_date#115, CASE WHEN customer_id#111L IN (customer_id#121L) THEN false ELSE current#131 END AS current#138]\n+- Project [customer_id#111L, name#112, address#113, phone#114, effective_date#115, true AS current#131]\n   +- LogicalRDD [customer_id#111L, name#112, address#113, phone#114, effective_date#115], false\n","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n","File \u001B[0;32m<command-1361810498471495>:23\u001B[0m\n","\u001B[1;32m     21\u001B[0m new_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(new_data, [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcustomer_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124maddress\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mphone\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meffective_date\u001B[39m\u001B[38;5;124m'\u001B[39m])\n","\u001B[1;32m     22\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcurrent\u001B[39m\u001B[38;5;124m'\u001B[39m, lit(\u001B[38;5;28;01mTrue\u001B[39;00m))\n","\u001B[0;32m---> 23\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcurrent\u001B[39m\u001B[38;5;124m'\u001B[39m, when(df\u001B[38;5;241m.\u001B[39mcustomer_id\u001B[38;5;241m.\u001B[39misin(new_df\u001B[38;5;241m.\u001B[39mcustomer_id), \u001B[38;5;28;01mFalse\u001B[39;00m)\u001B[38;5;241m.\u001B[39motherwise(df\u001B[38;5;241m.\u001B[39mcurrent))\n","\u001B[1;32m     24\u001B[0m df\u001B[38;5;241m.\u001B[39mcollect()\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n","\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n","\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n","\u001B[1;32m     51\u001B[0m     )\n","\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4758\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n","\u001B[1;32m   4753\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n","\u001B[1;32m   4754\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n","\u001B[1;32m   4755\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n","\u001B[1;32m   4756\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n","\u001B[1;32m   4757\u001B[0m     )\n","\u001B[0;32m-> 4758\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n","\n","File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n","\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n","\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n","\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n","\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n","\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n","\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n","\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n","\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n","\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n","\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n","\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n","\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n","\n","\u001B[0;31mAnalysisException\u001B[0m: Resolved attribute(s) customer_id#121L missing from customer_id#111L,name#112,address#113,phone#114,effective_date#115,current#131 in operator !Project [customer_id#111L, name#112, address#113, phone#114, effective_date#115, CASE WHEN customer_id#111L IN (customer_id#121L) THEN false ELSE current#131 END AS current#138]. Attribute(s) with the same name appear in the operation: customer_id. Please check if the right attribute(s) are used.;\n","!Project [customer_id#111L, name#112, address#113, phone#114, effective_date#115, CASE WHEN customer_id#111L IN (customer_id#121L) THEN false ELSE current#131 END AS current#138]\n","+- Project [customer_id#111L, name#112, address#113, phone#114, effective_date#115, true AS current#131]\n","   +- LogicalRDD [customer_id#111L, name#112, address#113, phone#114, effective_date#115], false\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import current_date, lit, when\n\n# Create a Spark session\nspark = SparkSession.builder.getOrCreate()\n\n# Sample data with customer information\ndata = [\n    (1, 'John Doe', '123 Main St', '555-1234', '2021-01-01'),\n    (2, 'Jane Smith', '456 Elm St', '555-5678', '2021-01-01'),\n    (3, 'Bob Johnson', '789 Oak St', '555-9090', '2021-01-01')\n]\n\n# Create an initial DataFrame\ndf = spark.createDataFrame(data, ['customer_id', 'name', 'address', 'phone', 'effective_date'])\n\n# New data to update the address for customer_id 2\nnew_data = [\n    (2, 'Jane Smith', '789 Pine St', '555-5678', '2023-06-19')\n]\nnew_df = spark.createDataFrame(new_data, ['customer_id', 'name', 'address', 'phone', 'effective_date'])\n\n# Create a column to represent the current record\ndf = df.withColumn('current', lit(True))\njoined_df = df.join(new_df, 'customer_id', 'left_outer')\njoined_df.collect()\njoined_df.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d5a64c98-a750-47a8-9179-858358d5e7f8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[13]: [Row(customer_id=1, name='John Doe', address='123 Main St', phone='555-1234', effective_date='2021-01-01', current=True, name=None, address=None, phone=None, effective_date=None),\n Row(customer_id=2, name='Jane Smith', address='456 Elm St', phone='555-5678', effective_date='2021-01-01', current=True, name='Jane Smith', address='789 Pine St', phone='555-5678', effective_date='2023-06-19'),\n Row(customer_id=3, name='Bob Johnson', address='789 Oak St', phone='555-9090', effective_date='2021-01-01', current=True, name=None, address=None, phone=None, effective_date=None)]"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dc6372de-57ba-46a5-94ed-aacdcfb2f036","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Untitled Notebook 2023-06-19 17:11:39","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
